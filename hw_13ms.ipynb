{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jZGCbdRK-rs",
    "outputId": "3ba28e30-3faa-426a-84d2-54eefadf3a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: matplotlib_inline in /usr/local/lib/python3.8/dist-packages (0.1.6)\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.8/dist-packages (from matplotlib_inline) (5.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "n6S-j78fLcgH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "7Umd2tB3bgu2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "import scipy.stats as ss\n",
    "from scipy import optimize\n",
    "from scipy.optimize import minimize, LinearConstraint, NonlinearConstraint\n",
    "import pandas as pd\n",
    "from scipy.integrate import quad\n",
    "from itertools import product\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obRWTdkXFyyW"
   },
   "source": [
    "# Ответы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWufW2YljBAL"
   },
   "source": [
    "### 1.\n",
    "\n",
    "Установите np.random.seed(1234) вместо np.random.seed(1337). С чем связано различие в результатах? Что можно сделать, чтобы это различие уменьшилось?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDK4tltkjGpF"
   },
   "source": [
    "У нас очень маленькая выборка и шум сильно влияет на обучение, нам не повезло с выборкой и все стало очень плохо. Если увеличить размер $n_{obs}$, то наши предсказания будут более устойчивы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epprf2FMHcSv"
   },
   "source": [
    "### 2.\n",
    "\n",
    "Нарисуйте график для a = 5, b = 1. Почему синяя кривая перестала быть близкой к f(x) при x < 0.4, ведь мы знаем, что если обучаемая модель верна (а она верна: $f(x) = 1 + 5x + 0x^2 + 0x^3 = f^*(x, \\beta = (1, 5, 0, 0))f(x)=1+5x+0x \n",
    "2\n",
    " +0x \n",
    "3\n",
    " =f \n",
    "∗\n",
    " (x,β=(1,5,0,0)))$, то оценка МНК несмещенная и ее матожидание не зависит от распределения X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XnouOj1rHYQC",
    "outputId": "2332890c-baf3-48b5-f98e-80f1e6d510f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9021920238235627"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = 5, 1\n",
    "x_dist = ss.beta(a, b)\n",
    "(1 - x_dist.cdf(0.4)) ** 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RrAHfrKH6Nh"
   },
   "source": [
    "Синяя кривая всего лишь оценка посчитанная в каждой точке как:\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{j=1}^{n_{reps}} f^{*}(\\beta^{*}_j, x)}{n_{reps}}\n",
    "$$\n",
    "\n",
    "При этом $x$ распределены как $B(5, 1)$ и вероятность того, что все точки в датасете будут $ >0.4$ равна 0.9021. Это значит, что в среднем примерно 900 датасетов из 1000 не содержат ни одной точки меньше $0.4$, и мы никак не контролируем вклад, который делает $f^{*}(\\beta^{*}, x)$, обученная на таком датасете.\n",
    "\n",
    "Тоесть матожидание МНК-оценки несмещено и не зависит от распределения $X$.\n",
    "Но наша оценка этого матожидания уже зависит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW_QRoaWKnNM"
   },
   "source": [
    "### 3.\n",
    "\n",
    "Нарисуйте график для a = 5, b = 1. Почему оценка смещения близка к нулю несмотря на то, что синяя кривая сильно уклоняется от $f(x)$ при $x < 0.4$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RICeGJKIPmwv"
   },
   "source": [
    "Когда мы оцениваем $\\mathbb{E}(bias^2(X))$ мы считаем взвешенные суммы, учитывая изначальное распределение $X$ и вклад $f^{*}$ обученных на плохих датасетах нейтрализуется. Эта оценка выглядит так:\n",
    "\n",
    "$$\n",
    "\\sum_i w_ibias^2(x_i) = \\frac{\\sum_{i, j}w_if^{*}(\\beta^{*}_j, x_i)}{n_{reps}}\n",
    "$$\n",
    "\n",
    "Мы,в принципе, ответы всех моделей на $x_i < 0.4$ учитываем с весом меньше чем $10^{-4}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qf72CanQSJ30",
    "outputId": "b7773c05-eea4-4f42-f298-1eae65e52fe5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001270421857323759"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = 5, 1\n",
    "x_dist = ss.beta(a, b)\n",
    "x = np.linspace(0, 1, 1000)\n",
    "x_ = x[x < 0.4]\n",
    "x_dist.pdf(x_[-1]) / x_dist.pdf(x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lR3lBLS7Tbhd"
   },
   "source": [
    "### 4.\n",
    "\n",
    "Нарисуйте график для n_obs = 30. Что изменилось по сравнению с исходным графиком?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOm3KeGxUQls"
   },
   "source": [
    "Значительно уменьшились $\\mathbb{E}(bias^2(X))$ и $\\mathbb{E}(variance^2(X))$. Кривые стали точнее описывать нашу прямую: 1 + 5x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuP-zP6nX2Fg"
   },
   "source": [
    "### 5.\n",
    "\n",
    "Установите степень регуляризации alpha = 0.01. Что изменилось по сравнению с исходным графиком?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm7GAJOIX9dD"
   },
   "source": [
    "Взрывы на концах стали заметно меньше. Теперь мы обучаем кривые, которые точнее описывают прямую."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gl--QdX-YiL4"
   },
   "source": [
    "### 6.\n",
    "\n",
    "Установите степень регуляризации alpha = 10. Что изменилось по сравнению с исходным графиком? У $x \\approx 0$ истинная функция f(x) находится дальше от синей сплошной кривой, чем синяя пунктирная. Что это значит в терминах смещения и дисперсии?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY93gWIRZo2Q"
   },
   "source": [
    "Обученные кривые точнее предсказывают прямую $5x + 1$. Но, когда мы добавляем регуляризацию, МНК-оценка становится смещенной, что мы видим оценив $bias^2(x)$ для $x \\approx 0$. При этом $variance$ ожидаемо уменьшается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2Iyhx3rjGcA"
   },
   "source": [
    "# 7. \n",
    "\n",
    "Установите степень регуляризации $alpha = 10000$. Почему график так выглядит? К каким значениям стремятся $\\mathbb{E} \\mathrm{bias}^2(X)$ и $\\mathbb{E} \\mathrm{variance}(X)$ при $\\alpha \\to \\infty$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfmcTI_lj8tx"
   },
   "source": [
    "Теперь у нас слишком большой штраф за размер коэффициентов $\\beta^{*}$. И все наши $f^{*}(\\beta^{*}, x) \\approx \\overline{Y} \\rightarrow 3.5$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq73EwM5zG1A"
   },
   "source": [
    "При $\\alpha = \\infty$\n",
    "\n",
    " $\\beta^{*} = (\\overline{Y}, 0, 0, 0)$\n",
    "\n",
    " $\\overline{Y} = \\frac{1}{n} \\sum (5X_i + 1 + ɛ)$\n",
    "\n",
    " Пусть $D \\sim ([X]_{10}, [Y]_{10})$ -датасет.\n",
    "\n",
    " Тогда \n",
    " $$\n",
    " bias^2(x) = (1 + 5x - (1, x, x^2, x^3)\\mathbb{E}(β^{*}(D)))^2 = \n",
    " (1 + 5x - \\mathbb{E}(\\overline{Y}))^2 =\n",
    " (1 + 5x - \\mathbb{E}(1 + 5X))^2\n",
    " \\Rightarrow \n",
    " \\mathbb{E}(bias^2(x)) = \\mathbb{D}(1 + 5X) \\approx 2.08\n",
    " $$\n",
    "\n",
    " Пусть $n = n_{obs}$\n",
    "\n",
    " Тогда \n",
    "\n",
    " $$\n",
    " \\mathbb{D}((1, x, x^2, x^3)\\beta^{*}(D))\n",
    " = \\mathbb{D}(\\overline{Y}) =\n",
    " \\frac{1}{n^2}\\sum \\mathbb{D}(5X_i + 1 + ɛ) = \\frac{1}{n}\\mathbb{D}(5X + 1 + ɛ) =\n",
    " \\frac{1}{n}(\\mathbb{D}(5X + 1) + \\mathbb{D}(ɛ)) \\approx \\frac{2.08 + 1}{n} = \\frac{3.08}{n}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAuNp0s6Tbj5"
   },
   "source": [
    "# 8.\n",
    "\n",
    "Измените функцию f(x):\n",
    "\n",
    "    # Значение матожидания Y, если известно, что X = x\n",
    "    def f(x):\n",
    "    ____return 1 + 5 * (x > 0.5)\n",
    "\n",
    "    \n",
    "Нарисуйте графики для a = 6, b = 2; a = 4, b = 4; a = 2, b = 6. Объясните различия в синих кривых."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXgId2BOkHMI"
   },
   "source": [
    "В случаях со скошенной плотностью (a=6, b=2 и a=2 b=6) основное обучение происходит в области высокой плотности x, где f(x) константа. Поэтому мы пытаемся приблизить две различные прымые многочленами 3 степени. А в случае \n",
    "a=4, b = 4 основная масса x приходится в окрестность 0.5 и наша кривая пытается \n",
    "объяснить разрыв в точке 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPIVqIW7OcGL"
   },
   "source": [
    "# 9.\n",
    "\n",
    "Для $f(x)$ из предыдущего пункта нарисуйте график для $n_{obs}$ 10, 100 и 1000. Что меняется? К чему стремится функция $\\mathbb{E} f^*(x, \\beta^*)$ при $n \\to \\infty$(явный вид функции считать не надо; если хотите, можете посчитать его численно)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBDU9RiZi1DU"
   },
   "source": [
    "$$\n",
    "\\mathbb{E} f^*(x, \\beta^*) = (1, x, x^2, x^3)\\mathbb{E}(\\beta^*)\n",
    "$$\n",
    "\n",
    "При бесконечной выборке $\\beta^*$, такое что $f^*(x, \\beta^*)$ лучшая функция в классе $f^*(x, \\beta^*) = (1, x, x^2, x^3) \\beta^*)$, которая приближает $L^2$\n",
    "норму функции $f(x)$. Чтобы найти $\\mathbb{E}{\\beta^*}$ нужно найти:\n",
    "\n",
    "$$\n",
    "argmin_{\\beta} \\int_0^1 (f(x) - f^*(x, \\beta))^2 dx\n",
    "$$\n",
    "\n",
    "Если минимизировать численно, то получится:\n",
    "$\\beta$ =  (1.93750008 -18.7500046   65.62501378 -43.75000942)\n",
    "\n",
    "и \n",
    "\n",
    "$$\n",
    "\\mathbb{E} f^*(x, \\beta^*) \\approx 1.93 - 18.75x + 65.62x^2 -43.75x^3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6si9K3dpaBOI",
    "outputId": "c3dd5f8f-1421-4993-ad96-0d779223f384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E(beta*) = [  1.93750008 -18.7500046   65.62501378 -43.75000942]\n"
     ]
    }
   ],
   "source": [
    "import scipy.integrate as integrate\n",
    "import scipy.special as special\n",
    "\n",
    "def f_star(x, beta):\n",
    "  x_ = np.array([1, x, x**2, x**3])\n",
    "  return (x_ * beta).sum()\n",
    "\n",
    "def f(x):\n",
    "      # Значение матожидания Y, если известно, что X = x\n",
    "      return 1 + 5 * (x > 0.5)\n",
    "\n",
    "\n",
    "def mse(beta):\n",
    "  result = integrate.quad(lambda x: (f(x) - f_star(x, beta))**2, 0, 1)\n",
    "  return result[0]\n",
    "\n",
    "\n",
    "bounds=[(None, None) for _ in range(4)]\n",
    "beta0 = np.zeros(4)\n",
    "\n",
    "res = minimize(lambda beta: mse(beta),x0=beta0, bounds=bounds)\n",
    "print('E(beta*) =', res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mjn3UmlZTJz-"
   },
   "source": [
    "# 10.\n",
    "\n",
    "\n",
    "Измените функцию f(x):\n",
    "\n",
    "    # Значение матожидания Y, если известно, что X = x\n",
    "    def f(x):\n",
    "    ___return 1 + 5 * np.sin(np.pi * x)\n",
    "\n",
    "Почему смещение такое маленькое? Функция $1 + 5\\sin(\\pi x)$ ведь не является кубической, то есть $f^*(x, \\beta)$ ни при каких значениях $\\beta$ не равна $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5rbmEQJlO70"
   },
   "source": [
    "На промежутке $[0,1]$ $1+5sin(πx)$ имеет всего 1 экстремум поэтому она хорошо приближается своим многочленом Тейлора 3-ей степени на всем отрезке, его и приближает наша регрессия."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "epprf2FMHcSv",
    "KW_QRoaWKnNM",
    "lR3lBLS7Tbhd",
    "LuP-zP6nX2Fg",
    "Gl--QdX-YiL4",
    "_2Iyhx3rjGcA",
    "hAuNp0s6Tbj5",
    "HPIVqIW7OcGL"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
